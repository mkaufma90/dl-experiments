{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import spacy\n",
    "from scipy import spatial\n",
    "from collections import defaultdict\n",
    "nlp = spacy.load('en')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the corpus, and convert it into training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open('./alice_in_wonderland.txt').read()\n",
    "doc = nlp(text.decode('utf8'))\n",
    "\n",
    "#we don't really care about training on all deps \n",
    "bad_deps = ['ROOT', 'compound', 'pobj', 'punct']\n",
    "examples = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    for word in sent:\n",
    "        #casing should probably be a paramater\n",
    "        source =  word.head.text.lower()\n",
    "        target= word.text.lower()\n",
    "        dep = word.dep_\n",
    "        for child in word.children:\n",
    "            source =  child.head.text.lower()\n",
    "            target= child.text.lower()\n",
    "            dep = child.dep_\n",
    "            #If we see a prepositional dependency, we want to merge it\n",
    "            #so ('scientist', prep, 'with') and ('with', pobj,'telescope) \n",
    "            #becomes ('scientist, 'prep_with', 'telescope)\n",
    "            if dep == 'prep':\n",
    "                for c2 in child.children:\n",
    "                    if (c2.dep_ == 'pobj'):\n",
    "                        examples.append((source,\"prep_\" + child.text.lower(),  c2.text))\n",
    "            else:\n",
    "                if not dep in bad_deps:\n",
    "                    examples.append((source, dep,target))\n",
    "#index all depndency triples by their head, so we can sort them in different ways \n",
    "indexed_training = defaultdict(list)\n",
    "for (a,b,c)in examples:\n",
    "    indexed_training[a].append((b,c))\n",
    "\n",
    "\n",
    "def convert_predict_context(indexed_training):\n",
    "    #x and y are training data and labels (respectively)\n",
    "    #If reverse is true, we the pair (training, dep, context) will generate (training, dep_context)\n",
    "    #and (context, dep_training)\n",
    "    #TODO, should the reversed example be treated differently? That is done in the original paper\n",
    "    #but no empirical justification is given\n",
    "    x = []\n",
    "    y = []\n",
    "    reverse = True\n",
    "    for k,Vs in indexed_training.iteritems():\n",
    "        for (context_dep, context_word) in Vs:\n",
    "            x.append(context_dep +\"_\" + context_word)\n",
    "            y.append(k)\n",
    "        if reverse:\n",
    "            for (context_dep, context_word) in Vs:\n",
    "                x.append(context_dep +\"_\" + k)\n",
    "                y.append(context_word) \n",
    "\n",
    "\n",
    "    train_indices = dict((w, i) for i, w in enumerate(set(x)))\n",
    "    label_indices = dict((w, i) for i, w in enumerate(set(y)))\n",
    "    #y gets converted to one_hot vectors, but we can leave x as the indeces\n",
    "    #because of the Embedding layer\n",
    "    x = np.array([train_indices[w] for w in x])\n",
    "    y = np.array([label_indices[w] for w in y])\n",
    "    y = np_utils.to_categorical(y, np.argmax(y))\n",
    "    V = np.argmax(y)\n",
    "    return (x,y,train_indices,label_indices)\n",
    "\n",
    "\n",
    "\n",
    "def convert_predict_words(indexed_training):\n",
    "    #x and y are training data and labels (respectively)\n",
    "    #If reverse is true, we the pair (training, dep, context) will generate (training, dep_context)\n",
    "    #and (context, dep_training)\n",
    "    #TODO, should the reversed example be treated differently? That is done in the original paper\n",
    "    #but no empirical justification is given\n",
    "    x = []\n",
    "    y = []\n",
    "    reverse = True\n",
    "    for k,Vs in indexed_training.iteritems():\n",
    "        for (context_dep, context_word) in Vs:\n",
    "            y.append(context_dep +\"_\" + context_word)\n",
    "            x.append(k)\n",
    "        if reverse:\n",
    "            for (context_dep, context_word) in Vs:\n",
    "                y.append(context_dep +\"_\" + k)\n",
    "                x.append(context_word) \n",
    "\n",
    "\n",
    "    train_indices = dict((w, i) for i, w in enumerate(set(x)))\n",
    "    label_indices = dict((w, i) for i, w in enumerate(set(y)))\n",
    "    #y gets converted to one_hot vectors, but we can leave x as the indeces\n",
    "    #because of the Embedding layer\n",
    "    x = np.array([train_indices[w] for w in x])\n",
    "    y = np.array([label_indices[w] for w in y])\n",
    "    y = np_utils.to_categorical(y, np.argmax(y))\n",
    "    return (x,y,train_indices,label_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input=', 41847, 'output=', 25879)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(embeddings_initializer=\"glorot_uniform\", output_dim=300, input_dim=41847, input_length=1)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=25879, activation=\"softmax\", input_dim=300)`\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#TODO, does it matter that the input and output vectors use the same indeces? \n",
    "(x,y,train_indices,label_indicies) = convert_predict_context(indexed_training)\n",
    "num_input = np.argmax(x)\n",
    "num_output = len(y[0])\n",
    "print(\"input=\",num_input,\"output=\",num_output)\n",
    "#300 dimensions for output, because thats what the paper does \n",
    "dim = 300\n",
    "#Embed into a 'dim' dimensional space, flatten it to 1d for output, then softmax it\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_input, output_dim=dim, init='glorot_uniform', input_length=1))\n",
    "model.add(Reshape((dim, )))\n",
    "model.add(Dense(input_dim=dim, output_dim=num_output, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "57090/57090 [==============================] - 19s - loss: 9.6485    \n",
      "Epoch 2/20\n",
      "57090/57090 [==============================] - 19s - loss: 7.3378    \n",
      "Epoch 3/20\n",
      "57090/57090 [==============================] - 19s - loss: 6.2395    \n",
      "Epoch 4/20\n",
      "57090/57090 [==============================] - 19s - loss: 5.6572    \n",
      "Epoch 5/20\n",
      "57090/57090 [==============================] - 18s - loss: 5.2705    \n",
      "Epoch 6/20\n",
      "57090/57090 [==============================] - 18s - loss: 4.9627    \n",
      "Epoch 7/20\n",
      "57090/57090 [==============================] - 19s - loss: 4.6922    \n",
      "Epoch 8/20\n",
      "57090/57090 [==============================] - 18s - loss: 4.4468    \n",
      "Epoch 9/20\n",
      "57090/57090 [==============================] - 19s - loss: 4.2117    \n",
      "Epoch 10/20\n",
      "57090/57090 [==============================] - 18s - loss: 3.9840    \n",
      "Epoch 11/20\n",
      "57090/57090 [==============================] - 18s - loss: 3.7633    \n",
      "Epoch 12/20\n",
      "57090/57090 [==============================] - 19s - loss: 3.5501    \n",
      "Epoch 13/20\n",
      "57090/57090 [==============================] - 19s - loss: 3.3467    \n",
      "Epoch 14/20\n",
      "57090/57090 [==============================] - 19s - loss: 3.1565    \n",
      "Epoch 15/20\n",
      "57090/57090 [==============================] - 19s - loss: 2.9815    \n",
      "Epoch 16/20\n",
      "57090/57090 [==============================] - 19s - loss: 2.8246    \n",
      "Epoch 17/20\n",
      "57090/57090 [==============================] - 19s - loss: 2.6856    \n",
      "Epoch 18/20\n",
      "57090/57090 [==============================] - 18s - loss: 2.5649    \n",
      "Epoch 19/20\n",
      "57090/57090 [==============================] - 18s - loss: 2.4623    \n",
      "Epoch 20/20\n",
      "57090/57090 [==============================] - 19s - loss: 2.3780    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f969f08a0d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y,batch_size=512, epochs=20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, train_indices):\n",
    "    train_indices_rev = {b : a for a,b in train_indices.iteritems()}\n",
    "    embeddings = model.get_weights()[0]\n",
    "    vecs = dict()\n",
    "    for i,w in train_indices_rev.iteritems():\n",
    "        vecs[i] = embeddings[i]\n",
    "    count = 0\n",
    "    for input_word in train_indices.keys()[0:10]:\n",
    "        input_vector = embeddings[train_indices[input_word]]\n",
    "\n",
    "        sims = dict()\n",
    "        for idx,vector in vecs.iteritems():\n",
    "            result = 1 - spatial.distance.cosine(vector, input_vector)\n",
    "            sims[train_indices_rev[idx]] = result \n",
    "\n",
    "        print(input_word)\n",
    "        for (a,b) in sorted(sims.items(), key=lambda x:x[1], reverse=True)[:10]:\n",
    "            print(\"\\t\" + str(a) + \": \" + str(b))\n",
    "        \n",
    "        count += 1\n",
    "        if (count > 10):\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prep_in_sleep\n",
      "\tprep_in_sleep: 0.999999918645\n",
      "\tconj_giving: 0.863054418029\n",
      "\tacl_dear: 0.78758052845\n",
      "\tprep_of_axes: 0.78692723477\n",
      "\tadvmod_familiarly: 0.786467974289\n",
      "\tconj_smiling: 0.785594899253\n",
      "\tadvcl_gloves: 0.784982644274\n",
      "\tprep_in_manner: 0.783622716608\n",
      "\tnsubj_recognised: 0.781431259298\n",
      "\tconj_nursing: 0.777836998244\n",
      "prt_got\n",
      "\tprt_got: 1.00000005935\n",
      "\tprt_marked: 0.978079096881\n",
      "\tprt_held: 0.969610789246\n",
      "\tprt_made: 0.950130742397\n",
      "\tprt_look: 0.929567484811\n",
      "\tprt_gazing: 0.910770170009\n",
      "\tprt_brightened: 0.909942484709\n",
      "\tprt_keeping: 0.909757633053\n",
      "\tprep_in_spite: 0.909645872802\n",
      "\tprt_picked: 0.9095285719\n",
      "prep_on_sides\n",
      "\tprep_on_sides: 0.999999913227\n",
      "\tauxpass_being: 0.680741016712\n",
      "\tagent_by: 0.662744532987\n",
      "\tprep_like_fish: 0.584138588749\n",
      "\tdobj_pieces: 0.578506348083\n",
      "\tconj_picked: 0.577769585363\n",
      "\tprep_to_puppy: 0.576482361297\n",
      "\tconj_difficulty: 0.576200802656\n",
      "\tpcomp_on: 0.5509562169\n",
      "\tdobj_it: 0.545201036797\n",
      "conj_director\n",
      "\tconj_director: 0.999999918836\n",
      "\tappos_gbnewby@pglaf.org: 0.99821548949\n",
      "\tappos_section: 0.997545523934\n",
      "\tappos_newby: 0.767821109945\n",
      "\tconj_northumbria: 0.520512271349\n",
      "\tconj_northumbria--: 0.509663919589\n",
      "\tconj_mustard: 0.477874902576\n",
      "\tcc_and: 0.477279353617\n",
      "\tprep_with_goose: 0.465370683365\n",
      "\tprep_as_m: 0.464134466863\n",
      "nummod_foot\n",
      "\tnummod_foot: 0.999999885589\n",
      "\tnummod_eye: 0.997943324258\n",
      "\tnummod_way: 0.997734826865\n",
      "\tnummod_hand: 0.997656613044\n",
      "\tnummod_finger: 0.997401494139\n",
      "\tnummod_knee: 0.996858449867\n",
      "\tnummod_side: 0.996796841942\n",
      "\tnummod_end: 0.996703096135\n",
      "\tprep_of_cakes: 0.995316427516\n",
      "\tdobj_drive: 0.992392387455\n",
      "cc_off\n",
      "\tcc_off: 0.999999873907\n",
      "\tcc_charities: 0.994894955484\n",
      "\tcc_kind: 0.994506044631\n",
      "\tcc_forgotten: 0.994494916561\n",
      "\tcc_horse: 0.994485971537\n",
      "\tcc_agree: 0.994483431374\n",
      "\tcc_digging: 0.994462229053\n",
      "\tcc_fallen: 0.994450364849\n",
      "\tcc_high: 0.994445921821\n",
      "\tcc_won: 0.994414504328\n",
      "_children\n",
      "\t_children: 0.999999899202\n",
      "\t_short: 0.995289141602\n",
      "\t_melancholy: 0.995155156656\n",
      "\t_making: 0.995144852666\n",
      "\t_--said: 0.995125764523\n",
      "\t_rattling: 0.995120049942\n",
      "\t_locations: 0.995095700081\n",
      "\t_go: 0.995050939978\n",
      "\t_sorrowful: 0.995023050396\n",
      "\t_they: 0.995016553025\n",
      "cc_hour\n",
      "\tcc_hour: 0.999999873079\n",
      "\tcc_once: 0.995432456433\n",
      "\tcc_copy: 0.995225593532\n",
      "\tcc_something: 0.993923401436\n",
      "\tcc_two: 0.993800802588\n",
      "\tcc_on: 0.993674995026\n",
      "\tcc_copying: 0.99320491637\n",
      "\tcc_cats: 0.993162591447\n",
      "\tcc_with: 0.993138274038\n",
      "\tcc_pig: 0.993053740175\n",
      "nsubj_pronounced\n",
      "\tnsubj_pronounced: 0.999999933737\n",
      "\tnsubj_wore: 0.996774584093\n",
      "\tnsubj_trims: 0.996210563326\n",
      "\tnsubj_trusts: 0.996202623294\n",
      "\tnsubj_enjoy: 0.996150894376\n",
      "\tnsubj_consented: 0.996113946492\n",
      "\tnsubj_poured: 0.995851856874\n",
      "\tnsubj_thanked: 0.995826444636\n",
      "\tnsubj_sent: 0.995818295553\n",
      "\tnsubj_treading: 0.995798551756\n",
      "prt_blown\n",
      "\tprt_blown: 0.99999994681\n",
      "\tprt_thrown: 0.996397191909\n",
      "\tprep_on_be: 0.996212685032\n",
      "\tprt_turns: 0.995824175569\n",
      "\tprt_shouted: 0.995727574497\n",
      "\tprt_bursting: 0.995679164071\n",
      "\tprt_tired: 0.995622457798\n",
      "\tprt_straightening: 0.995618416634\n",
      "\tprep_of_wood--(she: 0.995605016916\n",
      "\tprep_of_pocket: 0.995560234564\n"
     ]
    }
   ],
   "source": [
    "print_top_words(model,train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
